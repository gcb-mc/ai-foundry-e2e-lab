{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f0b7d7",
   "metadata": {},
   "source": [
    "# üçè Observability & Tracing Demo with `azure-ai-projects` and `azure-ai-inference` üçé\n",
    "\n",
    "> üìö **For developers and learners**: Refer to the official Azure AI Foundry observability documentation: [https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/observability](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/observability)\n",
    "\n",
    "Welcome to this **Health & Fitness**-themed notebook, where we'll explore how to set up **observability** and **tracing** for:\n",
    "\n",
    "1. **Basic LLM calls** using an `AIProjectClient`.\n",
    "2. **Multi-step** interactions using an **Agent** (such as a Health Resource Agent).\n",
    "3. **Tracing** your local usage in **console** (stdout) or via an **OTLP endpoint** (like **Prompty** or **Aspire**).\n",
    "4. Sending those **traces** to **Azure Monitor** (Application Insights) so you can view them in **Azure AI Foundry**.\n",
    "\n",
    "> **Disclaimer**: This is a fun demonstration of AI and observability! Any references to workouts, diets, or health routines in the code or prompts are purely for **educational** purposes. Always consult a professional for health advice.\n",
    "\n",
    "## Contents\n",
    "1. **Initialization**: Setting up environment, creating clients.\n",
    "2. **Basic LLM Call**: Quick demonstration of retrieving model completions.\n",
    "3. **Connections**: Listing project connections.\n",
    "4. **Observability & Tracing**\n",
    "   - **Azure Monitor** tracing: hooking up to Application Insights\n",
    "   - **Verifying** your traces in Azure AI Foundry\n",
    "5. **Agent-based Example**:\n",
    "   - Creating a simple \"Health Resource Agent\" referencing sample docs.\n",
    "   - Multi-turn conversation with tracing.\n",
    "   - Cleanup.\n",
    "\n",
    "<img src=\"./seq-diagrams/1-observability.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e13f9f3",
   "metadata": {},
   "source": [
    "## 1. Initialization & Setup\n",
    "**Prerequisites**:\n",
    "- A `.env` file containing `PROJECT_CONNECTION_STRING` (and optionally `MODEL_DEPLOYMENT_NAME`).\n",
    "- Roles/permissions in Azure AI Foundry that let you do inference & agent creation.\n",
    "- A local environment with `azure-ai-projects`, `azure-ai-inference`, `opentelemetry` packages installed.\n",
    "\n",
    "**What we do**:\n",
    "- Load environment variables.\n",
    "- Initialize `AIProjectClient`.\n",
    "- Check that we can talk to a model (like `gpt-4o`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ccdace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.inference.models import UserMessage, CompletionsFinishReason\n",
    "\n",
    "# Load environment variables\n",
    "notebook_path = Path().absolute()\n",
    "env_path = notebook_path.parent.parent / '.env'  # Adjust path as needed\n",
    "load_dotenv(env_path)\n",
    "\n",
    "connection_string = os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
    "tenant_id = os.environ.get(\"TENANT_ID\")\n",
    "if not connection_string:\n",
    "    raise ValueError(\"üö® PROJECT_CONNECTION_STRING not set in .env.\")\n",
    "\n",
    "print(f\"üîë Using Tenant ID: {tenant_id}\")\n",
    "\n",
    "# Initialize AIProjectClient with simplified browser-based authentication\n",
    "try:\n",
    "    print(\"üåê Using browser-based authentication to bypass Azure CLI cache issues...\")\n",
    "    \n",
    "    # Use only InteractiveBrowserCredential with the specific tenant\n",
    "    credential = InteractiveBrowserCredential(tenant_id=tenant_id)\n",
    "    \n",
    "    # Create the project client using endpoint (connection_string is actually the endpoint URL)\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=connection_string,\n",
    "        credential=credential\n",
    "    )\n",
    "    print(\"‚úÖ Successfully created AIProjectClient!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating AIProjectClient: {e}\")\n",
    "    print(\"üí° Please complete the browser authentication prompt that should appear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24461b",
   "metadata": {},
   "source": [
    "## 2. Basic LLM Call\n",
    "We'll do a **quick** chat completion request to confirm everything is working. We'll ask a simple question: \"How many feet are in a mile?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcdaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Use the direct inference approach with Azure OpenAI client\n",
    "    with project_client.inference.get_azure_openai_client(api_version=\"2024-10-21\") as openai_client:\n",
    "        # Default to \"gpt-4o\" if no env var is set\n",
    "        model_name = os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "\n",
    "        user_question = \"How many feet are in a mile?\"\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": user_question}]\n",
    "        )\n",
    "        print(\"\\nüí°Response:\")\n",
    "        print(response.choices[0].message.content)\n",
    "        print(\"\\nFinish reason:\", response.choices[0].finish_reason)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Could not complete the chat request:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b83517e",
   "metadata": {},
   "source": [
    "## 3. List & Inspect Connections\n",
    "Check out the **connections** your project has: these might be Azure OpenAI or other resource attachments. We'll just list them here for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70793c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import ConnectionType\n",
    "\n",
    "all_conns = list(project_client.connections.list())\n",
    "print(f\"üîé Found {len(all_conns)} total connections.\")\n",
    "for idx, c in enumerate(all_conns):\n",
    "    # Handle different possible attribute names\n",
    "    conn_type = getattr(c, 'connection_type', getattr(c, 'type', 'Unknown'))\n",
    "    endpoint = getattr(c, 'endpoint_url', getattr(c, 'endpoint', 'Unknown'))\n",
    "    print(f\"{idx+1}) Name: {c.name}, Type: {conn_type}, Endpoint: {endpoint}\")\n",
    "\n",
    "# Filter for Azure OpenAI connections\n",
    "try:\n",
    "    aoai_conns = list(project_client.connections.list(connection_type=ConnectionType.AZURE_OPEN_AI))\n",
    "    print(f\"\\nüåÄ Found {len(aoai_conns)} Azure OpenAI connections:\")\n",
    "    for c in aoai_conns:\n",
    "        print(f\"   -> {c.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nüåÄ Could not filter Azure OpenAI connections: {e}\")\n",
    "    # Try to find them manually\n",
    "    aoai_conns = [c for c in all_conns if 'openai' in str(getattr(c, 'connection_type', getattr(c, 'type', ''))).lower()]\n",
    "    print(f\"üåÄ Found {len(aoai_conns)} connections with 'openai' in type:\")\n",
    "    for c in aoai_conns:\n",
    "        print(f\"   -> {c.name}\")\n",
    "\n",
    "# Get default connection of type AZURE_AI_SERVICES\n",
    "try:\n",
    "    default_conn = project_client.connections.get_default(connection_type=ConnectionType.AZURE_AI_SERVICES,\n",
    "                                                         include_credentials=False)\n",
    "    if default_conn:\n",
    "        print(\"\\n‚≠ê Default Azure AI Services connection:\")\n",
    "        print(f\"   Name: {default_conn.name}\")\n",
    "        print(f\"   Type: {getattr(default_conn, 'connection_type', getattr(default_conn, 'type', 'Unknown'))}\")\n",
    "        print(f\"   Endpoint: {getattr(default_conn, 'endpoint_url', getattr(default_conn, 'endpoint', 'Unknown'))}\")\n",
    "    else:\n",
    "        print(\"\\nNo default connection found for Azure AI Services.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error getting default connection: {e}\")\n",
    "    print(\"üí° This might be expected if no default connection is configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce0c8f7",
   "metadata": {},
   "source": [
    "## 4. Observability & Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d366bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages exactly as specified in Microsoft documentation\n",
    "!pip install azure-ai-projects azure-monitor-opentelemetry opentelemetry-instrumentation-openai-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767143a",
   "metadata": {},
   "source": [
    "## 4.1 Enable OpenTelemetry for Azure AI Inference\n",
    "We set environment variables to ensure:\n",
    "1. **Prompt content** is captured (optional!)\n",
    "2. The **Azure SDK** uses OpenTelemetry as its tracing implementation.\n",
    "3. We call `AIInferenceInstrumentor().instrument()` and `OpenAIInstrumentor().instrument()` to patch and enable the instrumentation.\n",
    "\n",
    "\"## 4.1 Azure Monitor Tracing (Application Insights)\n",
    "\",\n",
    "        \"\n",
    "\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef06776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference.tracing import AIInferenceInstrumentor\n",
    "\n",
    "# (Optional) capture prompt & completion contents in traces - MUST be set before instrumentation\n",
    "os.environ[\"OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT\"] = \"true\"\n",
    "os.environ[\"AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED\"] = \"true\"  # Keep for backward compatibility\n",
    "\n",
    "# Let the Azure SDK know we want to use OpenTelemetry\n",
    "os.environ[\"AZURE_SDK_TRACING_IMPLEMENTATION\"] = \"opentelemetry\"\n",
    "\n",
    "# Microsoft's exact approach: Instrument OpenAI SDK first\n",
    "try:\n",
    "    from opentelemetry.instrumentation.openai_v2 import OpenAIInstrumentor\n",
    "    OpenAIInstrumentor().instrument()\n",
    "    print(\"‚úÖ OpenAI v2 instrumentation enabled (Microsoft approach).\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è opentelemetry-instrumentation-openai-v2 not available\")\n",
    "    print(\"üí° Installing required package...\")\n",
    "\n",
    "# Then instrument Azure AI Inference\n",
    "AIInferenceInstrumentor().instrument()\n",
    "print(\"‚úÖ Azure AI Inference instrumentation enabled.\")\n",
    "print(\"‚úÖ Content recording enabled for traces.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480fbc30",
   "metadata": {},
   "source": [
    "### 4.1.2 Point Traces to Console or Local OTLP\n",
    "The simplest is to pipe them to **stdout**. If you want to send them to **Prompty** or **Aspire**, specify the local OTLP endpoint URL (usually `\"http://localhost:4317\"` or similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Console tracing setup - following Microsoft's official approach\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter\n",
    "\n",
    "# Set up console tracing for local debugging\n",
    "try:\n",
    "    # Initialize tracer provider if not already done\n",
    "    if not hasattr(trace, '_TRACER_PROVIDER') or trace._TRACER_PROVIDER is None:\n",
    "        tracer_provider = TracerProvider()\n",
    "        trace.set_tracer_provider(tracer_provider)\n",
    "    \n",
    "    # Add console exporter to see traces in stdout using Microsoft's recommended approach\n",
    "    console_exporter = ConsoleSpanExporter()\n",
    "    simple_processor = SimpleSpanProcessor(console_exporter)\n",
    "    trace.get_tracer_provider().add_span_processor(simple_processor)\n",
    "    \n",
    "    print(\"‚úÖ Console telemetry tracing enabled (Microsoft official approach).\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not set up console tracing: {e}\")\n",
    "    print(\"üí° Proceeding without console tracing - telemetry may still work via other means\")\n",
    "\n",
    "try:\n",
    "    # Use the project client to get Azure OpenAI client - this is the recommended approach\n",
    "    with project_client.inference.get_azure_openai_client(api_version=\"2024-10-21\") as openai_client:\n",
    "        user_prompt = \"What's a simple 5-minute warmup routine?\"\n",
    "        local_resp = openai_client.chat.completions.create(\n",
    "            model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\"),\n",
    "            messages=[{\"role\": \"user\", \"content\": user_prompt}]\n",
    "        )\n",
    "        print(\"\\nü§ñ Response:\", local_resp.choices[0].message.content)\n",
    "        print(\"üîç Check console output above for trace information\")\n",
    "except Exception as exc:\n",
    "    print(f\"‚ùå Error in local-tracing example: {exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c0fdd4",
   "metadata": {},
   "source": [
    "## 4.1 Azure Monitor Tracing (Application Insights)\n",
    "Now we'll set up tracing to **Application Insights**, which will forward your logs to the **Azure AI Foundry** **Tracing** page.\n",
    "\n",
    "**Steps**:\n",
    "1. In AI Foundry, go to your project‚Äôs **Tracing** tab, attach (or create) an **Application Insights** resource.\n",
    "2. In code, call `project_client.telemetry.get_connection_string()` to retrieve the instrumentation key.\n",
    "3. Use `azure.monitor.opentelemetry.configure_azure_monitor(...)` with that connection.\n",
    "4. Make an inference call -> logs appear in the Foundry portal (and in Azure Monitor itself).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0207221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-monitor-opentelemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903423e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional Azure Monitor components for manual setup if needed\n",
    "%pip install azure-monitor-opentelemetry-exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ef671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Reset OpenTelemetry completely for clean Azure Monitor setup\n",
    "# Run this cell if you're getting \"Overriding provider\" errors\n",
    "\n",
    "print(\"üîÑ Performing complete OpenTelemetry reset...\")\n",
    "\n",
    "# Restart Python kernel approach - this is the most reliable method\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Remove OpenTelemetry modules from cache\n",
    "modules_to_remove = [module for module in sys.modules if module.startswith('opentelemetry')]\n",
    "for module in modules_to_remove:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "print(f\"‚úÖ Removed {len(modules_to_remove)} OpenTelemetry modules from cache\")\n",
    "print(\"üí° OpenTelemetry will be reinitialized on next import\")\n",
    "\n",
    "# Re-import fresh OpenTelemetry modules\n",
    "from opentelemetry import trace\n",
    "print(\"‚úÖ Fresh OpenTelemetry trace module imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552014a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from azure.ai.inference.models import UserMessage\n",
    "from opentelemetry import trace\n",
    "\n",
    "# Get Application Insights connection string - following Microsoft's recommended approach\n",
    "print(\"üîß Attempting to get Application Insights connection string...\")\n",
    "try:\n",
    "    connection_string = project_client.telemetry.get_connection_string()\n",
    "    if connection_string:\n",
    "        print(\"‚úÖ Found App Insights connection string!\")\n",
    "        print(f\"üîó Connection string format: {connection_string[:50]}...\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No connection string returned from project client.\")\n",
    "        connection_string = None\n",
    "except AttributeError as e:\n",
    "    print(f\"‚ö†Ô∏è Telemetry operations not available: {e}\")\n",
    "    connection_string = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error getting App Insights connection string: {e}\")\n",
    "    connection_string = None\n",
    "\n",
    "# Configure Azure Monitor following Microsoft's exact approach from the documentation\n",
    "if connection_string:\n",
    "    try:\n",
    "        print(\"üîß Configuring Azure Monitor with fresh OpenTelemetry...\")\n",
    "        \n",
    "        # Microsoft's simple approach with fresh modules\n",
    "        configure_azure_monitor(connection_string=connection_string)\n",
    "        print(\"‚úÖ Azure Monitor configured successfully!\")\n",
    "        \n",
    "    except Exception as config_error:\n",
    "        print(f\"‚ö†Ô∏è Auto-configuration failed: {config_error}\")\n",
    "        print(\"üí° Using manual Azure Monitor setup...\")\n",
    "        \n",
    "        try:\n",
    "            # Manual setup with explicit provider creation\n",
    "            from azure.monitor.opentelemetry.exporter import AzureMonitorTraceExporter\n",
    "            from opentelemetry.sdk.trace import TracerProvider\n",
    "            from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "            \n",
    "            # Create new tracer provider\n",
    "            tracer_provider = TracerProvider()\n",
    "            trace.set_tracer_provider(tracer_provider)\n",
    "            \n",
    "            # Add Azure Monitor exporter\n",
    "            azure_exporter = AzureMonitorTraceExporter(connection_string=connection_string)\n",
    "            span_processor = BatchSpanProcessor(azure_exporter)\n",
    "            tracer_provider.add_span_processor(span_processor)\n",
    "            \n",
    "            print(\"‚úÖ Manual Azure Monitor setup completed!\")\n",
    "            \n",
    "        except Exception as manual_error:\n",
    "            print(f\"‚ùå Manual setup failed: {manual_error}\")\n",
    "            print(\"üí° Proceeding with console-only tracing...\")\n",
    "            connection_string = None\n",
    "\n",
    "if connection_string:\n",
    "    print(\"\\nüß™ Testing Azure Monitor tracing with AI Foundry integration...\")\n",
    "    try:\n",
    "        # Try the correct method for getting Azure OpenAI client\n",
    "        try:\n",
    "            # Method 1: Direct get_azure_openai_client (newer API)\n",
    "            client = project_client.get_azure_openai_client()\n",
    "            print(\"‚úÖ Using project_client.get_azure_openai_client()\")\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                # Method 2: Through inference namespace (current working method)\n",
    "                client = project_client.inference.get_azure_openai_client(api_version=\"2024-10-21\")\n",
    "                print(\"‚úÖ Using project_client.inference.get_azure_openai_client()\")\n",
    "            except AttributeError:\n",
    "                # Method 3: Direct OpenAI client creation\n",
    "                from openai import AzureOpenAI\n",
    "                import os\n",
    "                \n",
    "                # Get connection details from project\n",
    "                aoai_conn = list(project_client.connections.list())[0]  # Use first available connection\n",
    "                client = AzureOpenAI(\n",
    "                    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "                    api_version=\"2024-10-21\",\n",
    "                    azure_endpoint=getattr(aoai_conn, 'endpoint', getattr(aoai_conn, 'endpoint_url', ''))\n",
    "                )\n",
    "                print(\"‚úÖ Using direct AzureOpenAI client\")\n",
    "        \n",
    "        # Create a custom span following Microsoft's example\n",
    "        from opentelemetry import trace\n",
    "        tracer = trace.get_tracer(__name__)\n",
    "        \n",
    "        with tracer.start_as_current_span(\"health_advice_request\") as span:\n",
    "            # Add custom attributes as shown in Microsoft docs\n",
    "            span.set_attribute(\"operation.type\", \"health_advice\")\n",
    "            span.set_attribute(\"operation.category\", \"fitness\")\n",
    "            \n",
    "            prompt_msg = \"Write a short poem about healthy living and exercise\"\n",
    "            response = client.chat.completions.create(\n",
    "                model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\"),\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt_msg}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Add response metadata to span\n",
    "            span.set_attribute(\"response.finish_reason\", response.choices[0].finish_reason)\n",
    "            span.set_attribute(\"response.model\", response.model)\n",
    "            if hasattr(response, 'usage') and response.usage:\n",
    "                span.set_attribute(\"gen_ai.usage.input_tokens\", response.usage.prompt_tokens)\n",
    "                span.set_attribute(\"gen_ai.usage.output_tokens\", response.usage.completion_tokens)\n",
    "            \n",
    "            print(\"\\nü§ñ Response (traced to AI Foundry):\")\n",
    "            print(response.choices[0].message.content)\n",
    "            \n",
    "        print(\"\\nüéØ Trace should now be visible in Azure AI Foundry portal!\")\n",
    "        print(\"   Navigate to: Your Project ‚Üí Tracing tab\")\n",
    "        print(\"   Look for a trace with operation name 'health_advice_request'\")\n",
    "        print(\"\\nüí° If traces still don't appear:\")\n",
    "        print(\"   1. Wait 2-5 minutes for propagation\")\n",
    "        print(\"   2. Check Application Insights directly in Azure Portal\")\n",
    "        print(\"   3. Verify OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT=true is set\")\n",
    "        print(\"   4. Ensure you're using the correct Application Insights resource\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Azure Monitor tracing test failed: {e}\")\n",
    "        import traceback\n",
    "        print(\"Full error details:\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\nüí° Azure Monitor tracing is not configured.\")\n",
    "    print(\"üîß To enable tracing in Azure AI Foundry:\")\n",
    "    print(\"   1. Go to https://ai.azure.com\")\n",
    "    print(\"   2. Navigate to your project\")\n",
    "    print(\"   3. Select 'Tracing' in the sidebar\")\n",
    "    print(\"   4. Connect or create an Application Insights resource\")\n",
    "    print(\"   5. Re-run this notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4991833",
   "metadata": {},
   "source": [
    "### 4.3 Viewing Traces in Azure AI Foundry\n",
    "After running the above code:\n",
    "1. Go to your AI Foundry project.\n",
    "2. Click **Tracing** on the sidebar.\n",
    "3. You should see the logs from your calls.\n",
    "4. Filter, expand, or explore them as needed.\n",
    "\n",
    "Also, if you want more advanced dashboards, you can open your **Application Insights** resource from the Foundry. In the App Insights portal, you get additional features like **end-to-end transaction** details, query logs, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dbb932",
   "metadata": {},
   "source": [
    "# 5. Agent-based Example\n",
    "We'll now create a **Health Resource Agent** that references sample docs about recipes or guidelines, then demonstrate:\n",
    "1. Creating an Agent with instructions.\n",
    "2. Creating a conversation thread.\n",
    "3. Running multi-step queries with **observability** enabled.\n",
    "4. Optionally cleaning up resources at the end.\n",
    "\n",
    "> The agent approach is helpful when you want more sophisticated conversation flows or **tool usage** (like file search)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ad934",
   "metadata": {},
   "source": [
    "## 5.1 Create Sample Files & Vector Store\n",
    "We'll create dummy `.md` files about recipes/guidelines, then push them into a **vector store** so our agent can do semantic search.\n",
    "\n",
    "(*This portion is a quick summary‚Äîsee [the other file-search tutorial] if you need more details.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_files():\n",
    "    \"\"\"Create some local .md files with sample text.\"\"\"\n",
    "    recipes_md = (\n",
    "        \"\"\"# Healthy Recipes Database\\n\\n\"\n",
    "        \"## Gluten-Free Recipes\\n\"\n",
    "        \"1. Quinoa Bowl\\n\"\n",
    "        \"   - Ingredients: quinoa, vegetables, olive oil\\n\"\n",
    "        \"   - Instructions: Cook quinoa, add vegetables\\n\\n\"\n",
    "        \"2. Rice Pasta\\n\"\n",
    "        \"   - Ingredients: rice pasta, mixed vegetables\\n\"\n",
    "        \"   - Instructions: Boil pasta, saut√© vegetables\\n\\n\"\n",
    "        \"## Diabetic-Friendly Recipes\\n\"\n",
    "        \"1. Low-Carb Stir Fry\\n\"\n",
    "        \"   - Ingredients: chicken, vegetables, tamari sauce\\n\"\n",
    "        \"   - Instructions: Cook chicken, add vegetables\\n\\n\"\n",
    "        \"## Heart-Healthy Recipes\\n\"\n",
    "        \"1. Baked Salmon\\n\"\n",
    "        \"   - Ingredients: salmon, lemon, herbs\\n\"\n",
    "        \"   - Instructions: Season salmon, bake\\n\\n\"\n",
    "        \"2. Mediterranean Bowl\\n\"\n",
    "        \"   - Ingredients: chickpeas, vegetables, tahini\\n\"\n",
    "        \"   - Instructions: Combine ingredients\\n\"\"\"\n",
    "    )\n",
    "\n",
    "    guidelines_md = (\n",
    "        \"\"\"# Dietary Guidelines\\n\\n\"\n",
    "        \"## General Guidelines\\n\"\n",
    "        \"- Eat a variety of foods\\n\"\n",
    "        \"- Control portion sizes\\n\"\n",
    "        \"- Stay hydrated\\n\\n\"\n",
    "        \"## Special Diets\\n\"\n",
    "        \"1. Gluten-Free Diet\\n\"\n",
    "        \"   - Avoid wheat, barley, rye\\n\"\n",
    "        \"   - Focus on naturally gluten-free foods\\n\\n\"\n",
    "        \"2. Diabetic Diet\\n\"\n",
    "        \"   - Monitor carbohydrate intake\\n\"\n",
    "        \"   - Choose low glycemic foods\\n\\n\"\n",
    "        \"3. Heart-Healthy Diet\\n\"\n",
    "        \"   - Limit saturated fats\\n\"\n",
    "        \"   - Choose lean proteins\\n\"\"\"\n",
    "    )\n",
    "\n",
    "    with open(\"recipes.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(recipes_md)\n",
    "    with open(\"guidelines.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(guidelines_md)\n",
    "\n",
    "    print(\"üìÑ Created sample resource files: recipes.md, guidelines.md\")\n",
    "    return [\"recipes.md\", \"guidelines.md\"]\n",
    "\n",
    "sample_files = create_sample_files()\n",
    "\n",
    "def create_vector_store(files, store_name=\"my_health_resources\"):\n",
    "    try:\n",
    "        uploaded_ids = []\n",
    "        for fp in files:\n",
    "            # Use the correct API pattern from the working notebooks\n",
    "            upl = project_client.agents.files.upload_and_poll(\n",
    "                file_path=fp,\n",
    "                purpose=\"assistants\"  # Use string instead of FilePurpose enum\n",
    "            )\n",
    "            uploaded_ids.append(upl.id)\n",
    "            print(f\"‚úÖ Uploaded: {fp} -> File ID: {upl.id}\")\n",
    "\n",
    "        # Create vector store from these file IDs using correct API pattern\n",
    "        vs = project_client.agents.vector_stores.create_and_poll(\n",
    "            file_ids=uploaded_ids,\n",
    "            name=store_name\n",
    "        )\n",
    "        print(f\"üéâ Created vector store '{store_name}', ID: {vs.id}\")\n",
    "        return vs, uploaded_ids\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating vector store: {e}\")\n",
    "        return None, []\n",
    "\n",
    "vector_store, file_ids = None, []\n",
    "if sample_files:\n",
    "    vector_store, file_ids = create_vector_store(sample_files, store_name=\"health_resources_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145eb186",
   "metadata": {},
   "source": [
    "## 5.2 Create a Health Resource Agent\n",
    "We'll create a **FileSearchTool** referencing the vector store, then create an agent with instructions that it should:\n",
    "1. Provide disclaimers.\n",
    "2. Offer general nutrition or recipe tips.\n",
    "3. Cite sources if possible.\n",
    "4. Encourage professional consultation for deeper medical advice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f604175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_health_agent(vs_id):\n",
    "    try:\n",
    "        # Create agent using the working pattern from file-search notebook\n",
    "        # Use tools as simple dictionary instead of FileSearchTool class\n",
    "        agent = project_client.agents.create_agent(\n",
    "            model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\"),\n",
    "            name=\"health-search-agent\",\n",
    "            instructions=\"\"\"\n",
    "                You are a health resource advisor with access to dietary and recipe files.\n",
    "                You:\n",
    "                1. Always present disclaimers (you're not a medical professional)\n",
    "                2. Provide references to files when possible\n",
    "                3. Focus on general nutrition or recipe tips.\n",
    "                4. Encourage professional consultation for more detailed advice.\n",
    "            \"\"\",\n",
    "            tools=[{\"type\": \"file_search\"}]  # Use dictionary instead of FileSearchTool class\n",
    "        )\n",
    "        print(f\"üéâ Created agent '{agent.name}' with ID: {agent.id}\")\n",
    "        print(\"üìã Vector store will be attached at message level for better compatibility\")\n",
    "        return agent\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating health agent: {e}\")\n",
    "        return None\n",
    "\n",
    "health_agent = None\n",
    "if vector_store:\n",
    "    health_agent = create_health_agent(vector_store.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6995a6",
   "metadata": {},
   "source": [
    "## 5.3 Using the Agent with Enhanced Tracing\n",
    "Let's create a new conversation **thread** and ask the agent some questions. We've enhanced this section to add **explicit tracing spans** around agent operations to ensure they appear in **Azure AI Foundry**.\n",
    "\n",
    "**Enhanced Tracing Features**:\n",
    "- Custom spans for agent demo, queries, and run execution\n",
    "- Detailed attributes including agent ID, thread ID, query text, and file attachment counts\n",
    "- Error tracking and status monitoring\n",
    "- Hierarchical span structure for better trace visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e5b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thread():\n",
    "    try:\n",
    "        # Use the correct API pattern for thread creation\n",
    "        thread = project_client.agents.threads.create()\n",
    "        print(f\"üìù Created new thread, ID: {thread.id}\")\n",
    "        return thread\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not create thread: {e}\")\n",
    "        return None\n",
    "\n",
    "def ask_question_with_files(thread_id, agent_id, user_question, file_ids_list):\n",
    "    try:\n",
    "        # Create explicit tracing span for agent operations\n",
    "        from opentelemetry import trace\n",
    "        tracer = trace.get_tracer(__name__)\n",
    "        \n",
    "        with tracer.start_as_current_span(\"agent_file_search_query\") as span:\n",
    "            # Add custom attributes for better tracing visibility\n",
    "            span.set_attribute(\"operation.type\", \"agent_file_search\")\n",
    "            span.set_attribute(\"agent.id\", agent_id)\n",
    "            span.set_attribute(\"thread.id\", thread_id)\n",
    "            span.set_attribute(\"query.text\", user_question)\n",
    "            span.set_attribute(\"file_attachments.count\", len(file_ids_list))\n",
    "            \n",
    "            # Create message with file attachments for search - using working pattern\n",
    "            attachments = []\n",
    "            for file_id in file_ids_list:\n",
    "                attachments.append({\n",
    "                    \"file_id\": file_id,\n",
    "                    \"tools\": [{\"type\": \"file_search\"}]\n",
    "                })\n",
    "            \n",
    "            # Add user message with file attachments\n",
    "            msg = project_client.agents.messages.create(\n",
    "                thread_id=thread_id,\n",
    "                role=\"user\",\n",
    "                content=user_question,\n",
    "                attachments=attachments\n",
    "            )\n",
    "            print(f\"User asked: '{user_question}' with {len(file_ids_list)} file attachments\")\n",
    "            \n",
    "            # Create & process a run with tracing\n",
    "            with tracer.start_as_current_span(\"agent_run_execution\") as run_span:\n",
    "                run_span.set_attribute(\"agent.model\", os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\"))\n",
    "                \n",
    "                run = project_client.agents.runs.create_and_process(\n",
    "                    thread_id=thread_id,\n",
    "                    agent_id=agent_id\n",
    "                )\n",
    "                \n",
    "                # Add run details to span\n",
    "                run_span.set_attribute(\"run.id\", run.id)\n",
    "                run_span.set_attribute(\"run.status\", run.status)\n",
    "                \n",
    "                print(f\"Run finished with status: {run.status}\")\n",
    "                if run.last_error:\n",
    "                    print(\"Error details:\", run.last_error)\n",
    "                    run_span.set_attribute(\"run.error\", str(run.last_error))\n",
    "                \n",
    "                return run\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error asking question: {e}\")\n",
    "        if 'span' in locals():\n",
    "            span.set_attribute(\"error\", True)\n",
    "            span.set_attribute(\"error.message\", str(e))\n",
    "        return None\n",
    "\n",
    "if health_agent:\n",
    "    # Create thread with tracing context\n",
    "    from opentelemetry import trace\n",
    "    tracer = trace.get_tracer(__name__)\n",
    "    \n",
    "    with tracer.start_as_current_span(\"health_agent_demo\") as demo_span:\n",
    "        demo_span.set_attribute(\"operation.type\", \"agent_demo\")\n",
    "        demo_span.set_attribute(\"agent.name\", \"health-search-agent\")\n",
    "        \n",
    "        thread = create_thread()\n",
    "        if thread:\n",
    "            # Let's ask a few sample questions with file search - all within tracing context\n",
    "            queries = [\n",
    "                \"Could you suggest a gluten-free lunch recipe?\",\n",
    "                \"Show me some heart-healthy meal ideas.\",\n",
    "                \"What guidelines do you have for someone with diabetes?\"\n",
    "            ]\n",
    "            \n",
    "            demo_span.set_attribute(\"queries.count\", len(queries))\n",
    "            \n",
    "            for idx, q in enumerate(queries):\n",
    "                if file_ids:\n",
    "                    with tracer.start_as_current_span(f\"query_{idx+1}\") as query_span:\n",
    "                        query_span.set_attribute(\"query.index\", idx + 1)\n",
    "                        query_span.set_attribute(\"query.text\", q)\n",
    "                        ask_question_with_files(thread.id, health_agent.id, q, file_ids)\n",
    "\n",
    "    print(\"\\nüéØ Agent traces should now be visible in Azure AI Foundry!\")\n",
    "    print(\"   Look for traces with operation names:\")\n",
    "    print(\"   - 'health_agent_demo' (overall demo)\")\n",
    "    print(\"   - 'agent_file_search_query' (each question)\")\n",
    "    print(\"   - 'agent_run_execution' (agent processing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c61d8d",
   "metadata": {},
   "source": [
    "### 5.3.1 Viewing the Conversation & Tracing Status\n",
    "We can retrieve the conversation messages to see how the agent responded, check if it cited file passages, and **verify that our tracing is working properly**.\n",
    "\n",
    "**üîç Tracing Troubleshooting**:\n",
    "\n",
    "If traces don't appear in Azure AI Foundry, check:\n",
    "\n",
    "1. **Application Insights Connection**: Ensure your AI Foundry project has an Application Insights resource connected\n",
    "2. **Connection String**: The `project_client.telemetry.get_connection_string()` should return a valid connection string\n",
    "3. **Time Delay**: Traces can take 2-5 minutes to appear in the portal\n",
    "4. **Trace Names**: Look for these specific operation names:\n",
    "   - `health_agent_demo` (overall agent demo)\n",
    "   - `agent_file_search_query` (each question asked)\n",
    "   - `agent_run_execution` (agent processing)\n",
    "   - `display_conversation` (conversation display)\n",
    "\n",
    "5. **Manual Verification**: Check Application Insights directly in Azure Portal if AI Foundry doesn't show traces\n",
    "6. **Environment Variables**: Ensure `OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT=true` is set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c57935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_thread(thread_id):\n",
    "    try:\n",
    "        # Add tracing context for message retrieval\n",
    "        from opentelemetry import trace\n",
    "        tracer = trace.get_tracer(__name__)\n",
    "        \n",
    "        with tracer.start_as_current_span(\"display_conversation\") as span:\n",
    "            span.set_attribute(\"operation.type\", \"display_thread\")\n",
    "            span.set_attribute(\"thread.id\", thread_id)\n",
    "            \n",
    "            # Use the correct API pattern for listing messages\n",
    "            messages = project_client.agents.messages.list(thread_id=thread_id)\n",
    "            print(\"\\nüó£Ô∏è Conversation:\")\n",
    "            \n",
    "            # Convert to list and reverse to show chronological order\n",
    "            message_list = list(messages)\n",
    "            span.set_attribute(\"messages.count\", len(message_list))\n",
    "            \n",
    "            for m in reversed(message_list):\n",
    "                if m.content:\n",
    "                    last_content = m.content[-1]\n",
    "                    if hasattr(last_content, \"text\"):\n",
    "                        print(f\"[{m.role.upper()}]: {last_content.text.value}\\n\")\n",
    "\n",
    "            print(\"\\nüìé Checking for citations...\")\n",
    "            citation_count = 0\n",
    "            for m in message_list:\n",
    "                if m.content:\n",
    "                    for content_item in m.content:\n",
    "                        if hasattr(content_item, \"text\") and hasattr(content_item.text, \"annotations\"):\n",
    "                            for annotation in content_item.text.annotations:\n",
    "                                citation_count += 1\n",
    "                                if hasattr(annotation, \"file_citation\"):\n",
    "                                    print(f\"- Citation {citation_count}: '{annotation.text}' from file ID: {annotation.file_citation.file_id}\")\n",
    "                                else:\n",
    "                                    print(f\"- Citation {citation_count}: '{annotation.text}'\")\n",
    "            \n",
    "            span.set_attribute(\"citations.count\", citation_count)\n",
    "            \n",
    "            if citation_count == 0:\n",
    "                print(\"No explicit citations found - checking for file content usage...\")\n",
    "                content_match_count = 0\n",
    "                for m in message_list:\n",
    "                    if m.role == \"assistant\" and m.content:\n",
    "                        for content_item in m.content:\n",
    "                            if hasattr(content_item, \"text\"):\n",
    "                                text = content_item.text.value.lower()\n",
    "                                if any(keyword in text for keyword in ['quinoa', 'salmon', 'gluten-free', 'diabetic', 'heart-healthy']):\n",
    "                                    print(f\"‚úÖ Agent appears to be using file content (found relevant keywords)\")\n",
    "                                    content_match_count += 1\n",
    "                                    break\n",
    "                \n",
    "                span.set_attribute(\"content_matches.count\", content_match_count)\n",
    "                                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not display thread: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Add error to span if it exists\n",
    "        if 'span' in locals():\n",
    "            span.set_attribute(\"error\", True)\n",
    "            span.set_attribute(\"error.message\", str(e))\n",
    "\n",
    "# If we created a thread above, let's read it with tracing\n",
    "if 'health_agent' in globals() and health_agent and 'thread' in globals() and thread:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä DISPLAYING CONVERSATION WITH TRACING\")\n",
    "    print(\"=\"*50)\n",
    "    display_thread(thread.id)\n",
    "    \n",
    "    print(\"\\nüîç **TRACING STATUS CHECK**:\")\n",
    "    print(f\"‚úÖ Azure Monitor connection: {'‚úì' if 'connection_string' in globals() and connection_string else '‚úó'}\")\n",
    "    print(f\"‚úÖ OpenTelemetry tracer: {'‚úì' if 'tracer' in globals() else '‚úó'}\")\n",
    "    print(f\"‚úÖ Agent operations traced: ‚úì (with custom spans)\")\n",
    "    print(f\"‚úÖ Thread ID for reference: {thread.id}\")\n",
    "    \n",
    "    print(\"\\nüéØ **WHERE TO FIND TRACES**:\")\n",
    "    print(\"1. Go to https://ai.azure.com\")  \n",
    "    print(\"2. Navigate to your project\")\n",
    "    print(\"3. Click 'Tracing' in the left sidebar\")\n",
    "    print(\"4. Look for these trace operation names:\")\n",
    "    print(\"   - 'health_agent_demo' (overall demo)\")\n",
    "    print(\"   - 'agent_file_search_query' (individual questions)\")\n",
    "    print(\"   - 'agent_run_execution' (agent processing)\")\n",
    "    print(\"   - 'display_conversation' (this display function)\")\n",
    "    print(\"5. Filter by time range if needed (last 15-30 minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7420c39",
   "metadata": {},
   "source": [
    "# 6. Cleanup\n",
    "If desired, we can remove the vector store, files, and agent to keep things tidy. (In a real solution, you might keep them around.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f473cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_resources():\n",
    "    try:\n",
    "        # Use the correct API pattern for cleanup\n",
    "        if 'vector_store' in globals() and vector_store:\n",
    "            project_client.agents.vector_stores.delete(vector_store.id)\n",
    "            print(\"üóëÔ∏è Deleted vector store.\")\n",
    "\n",
    "        if 'file_ids' in globals() and file_ids:\n",
    "            for fid in file_ids:\n",
    "                project_client.agents.files.delete(fid)\n",
    "            print(\"üóëÔ∏è Deleted uploaded files.\")\n",
    "\n",
    "        if 'health_agent' in globals() and health_agent:\n",
    "            project_client.agents.delete_agent(health_agent.id)\n",
    "            print(\"üóëÔ∏è Deleted health agent.\")\n",
    "\n",
    "        if 'sample_files' in globals() and sample_files:\n",
    "            for sf in sample_files:\n",
    "                if os.path.exists(sf):\n",
    "                    os.remove(sf)\n",
    "            print(\"üóëÔ∏è Deleted local sample files.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cleaning up: {e}\")\n",
    "\n",
    "\n",
    "cleanup_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956d0ec",
   "metadata": {},
   "source": [
    "# üéâ Wrap-Up\n",
    "We've demonstrated:\n",
    "1. **Basic LLM calls** with `AIProjectClient`.\n",
    "2. **Listing connections** in your Azure AI Foundry project.\n",
    "3. **Observability & tracing** in both local (console, OTLP endpoint) and cloud (App Insights) contexts.\n",
    "4. A quick **Agent** scenario that uses a vector store for searching sample docs.\n",
    "\n",
    "## Next Steps\n",
    "- Check the **Tracing** tab in your Azure AI Foundry portal to see the logs.\n",
    "- Explore advanced queries in Application Insights.\n",
    "- Use [Prompty](https://github.com/microsoft/prompty) or [Aspire](https://learn.microsoft.com/dotnet/aspire/) for local telemetry dashboards.\n",
    "- Incorporate this approach into your **production** GenAI pipelines!\n",
    "\n",
    "> üèãÔ∏è **Health Reminder**: The LLM's suggestions are for demonstration only. For real health decisions, consult a professional.\n",
    "\n",
    "Happy Observing & Tracing! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "end-to-end-ai-foundry-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "name": "Observability_and_Tracing_Comprehensive"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

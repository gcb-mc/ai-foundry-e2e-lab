{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02495886",
   "metadata": {},
   "source": [
    "## üìä Available Evaluators in Azure AI Foundry\n",
    "\n",
    "Azure AI Foundry provides a comprehensive set of built-in evaluators for different aspects of AI model quality:\n",
    "\n",
    "### **AI Quality (AI Assisted)**\n",
    "- **Groundedness** - Measures how well responses are grounded in provided context\n",
    "- **Relevance** - Evaluates how relevant responses are to the input query  \n",
    "- **Coherence** - Assesses logical flow and consistency in responses\n",
    "- **Fluency** - Measures language quality and readability\n",
    "- **GPT Similarity** - Compares responses to reference answers\n",
    "\n",
    "### **AI Quality (NLP Metrics)**\n",
    "- **F1 Score** - Measures precision and recall balance\n",
    "- **ROUGE Score** - Evaluates text summarization quality\n",
    "- **BLEU Score** - Measures translation and generation quality\n",
    "- **GLEU Score** - Google's BLEU variant for better correlation\n",
    "- **METEOR Score** - Considers synonyms and stemming\n",
    "\n",
    "### **Risk and Safety**\n",
    "- **Violence** - Detects violent content\n",
    "- **Sexual** - Identifies sexual content\n",
    "- **Self-harm** - Detects self-harm related content\n",
    "- **Hate/Unfairness** - Identifies hateful or unfair content\n",
    "- **Protected Material** - Detects copyrighted content\n",
    "- **Indirect Attack** - Identifies indirect prompt injection attempts\n",
    "\n",
    "üìö **For complete details on all available evaluators, their parameters, and usage examples, visit:**  \n",
    "**[Azure AI Foundry Evaluators Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/observability)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c0ebe",
   "metadata": {},
   "source": [
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è Azure AI Foundry Evaluations - Clean Version üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
    "\n",
    "This notebook demonstrates how to evaluate AI models using Azure AI Foundry with both **local** and **cloud** evaluations.\n",
    "\n",
    "## What This Notebook Does:\n",
    "1. **Setup & Data Creation** - Creates synthetic health & fitness Q&A data\n",
    "2. **Local Evaluation** - Runs F1Score and Relevance evaluators locally  \n",
    "3. **Cloud Evaluation** - Uploads results to Azure AI Foundry project\n",
    "\n",
    "## Key Features:\n",
    "‚úÖ **Local Evaluations** - F1Score and AI-assisted Relevance evaluators\n",
    "‚úÖ **Cloud Integration** - Upload results to Azure AI Foundry\n",
    "‚úÖ **Browser Authentication** - Uses InteractiveBrowserCredential  \n",
    "‚úÖ **Error Handling** - Robust fallbacks and clear status reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b889daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup and Data Creation\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Create synthetic health & fitness evaluation data\n",
    "synthetic_eval_data = [\n",
    "    {\n",
    "        \"query\": \"How can I start a beginner workout routine at home?\",\n",
    "        \"context\": \"Workout routines can include push-ups, bodyweight squats, lunges, and planks.\",\n",
    "        \"response\": \"You can just go for 10 push-ups total.\",\n",
    "        \"ground_truth\": \"At home, you can start with short, low-intensity workouts: push-ups, lunges, planks.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Are diet sodas healthy for daily consumption?\",\n",
    "        \"context\": \"Sugar-free or diet drinks may reduce sugar intake, but they still contain artificial sweeteners.\",\n",
    "        \"response\": \"Yes, diet sodas are 100% healthy.\",\n",
    "        \"ground_truth\": \"Diet sodas have fewer sugars than regular soda, but 'healthy' is not guaranteed due to artificial additives.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the capital of France?\",\n",
    "        \"context\": \"France is in Europe. Paris is the capital.\",\n",
    "        \"response\": \"London.\",\n",
    "        \"ground_truth\": \"Paris.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Write data to JSONL file\n",
    "eval_data_path = Path(\"./health_fitness_eval_data.jsonl\")\n",
    "with eval_data_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for row in synthetic_eval_data:\n",
    "        f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Evaluation data created: {eval_data_path.resolve()}\")\n",
    "print(f\"üìä Total samples: {len(synthetic_eval_data)}\")\n",
    "\n",
    "# Load environment variables\n",
    "current_dir = Path(os.getcwd())\n",
    "root_dir = current_dir.parent.parent if current_dir.name == \"3-quality_attributes\" else current_dir.parent.parent.parent\n",
    "load_dotenv(root_dir / '.env')\n",
    "print(\"‚úÖ Environment variables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d5598",
   "metadata": {
    "id": "3-Local-Evaluation"
   },
   "source": [
    "## üîç Local Evaluation\n",
    "\n",
    "Run evaluations locally using F1Score (basic text similarity) and Relevance (AI-assisted) evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f04f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Evaluation with Azure AI Foundry\n",
    "from azure.ai.evaluation import evaluate, F1ScoreEvaluator, RelevanceEvaluator\n",
    "import logging\n",
    "\n",
    "# Reduce logging noise\n",
    "logging.getLogger('promptflow').setLevel(logging.ERROR)\n",
    "logging.getLogger('azure.ai.evaluation').setLevel(logging.WARNING)\n",
    "\n",
    "print(\"üîç Running Local Evaluation...\")\n",
    "\n",
    "# Configure evaluators\n",
    "evaluators = {\n",
    "    \"f1_score\": F1ScoreEvaluator()\n",
    "}\n",
    "\n",
    "evaluator_config = {\n",
    "    \"f1_score\": {\n",
    "        \"column_mapping\": {\n",
    "            \"response\": \"${data.response}\",\n",
    "            \"ground_truth\": \"${data.ground_truth}\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add AI-assisted evaluator if Azure OpenAI is configured\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4\"),\n",
    "    \"api_version\": os.environ.get(\"AOAI_API_VERSION\", \"2024-02-15-preview\"),\n",
    "}\n",
    "\n",
    "if model_config[\"azure_endpoint\"] and model_config[\"api_key\"]:\n",
    "    print(\"ü§ñ Adding AI-assisted Relevance evaluator...\")\n",
    "    evaluators[\"relevance\"] = RelevanceEvaluator(model_config=model_config)\n",
    "    evaluator_config[\"relevance\"] = {\n",
    "        \"column_mapping\": {\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\"\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Azure OpenAI not configured - using F1Score only\")\n",
    "\n",
    "# Run local evaluation\n",
    "try:\n",
    "    local_result = evaluate(\n",
    "        data=str(eval_data_path),\n",
    "        evaluators=evaluators,\n",
    "        evaluator_config=evaluator_config\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Local evaluation completed!\")\n",
    "    \n",
    "    # Display results\n",
    "    metrics = local_result['metrics']\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"üìä {metric_name}: {value:.4f}\")\n",
    "        \n",
    "    # Save results locally\n",
    "    with open(\"local_evaluation_results.json\", \"w\") as f:\n",
    "        json.dump(local_result, f, indent=2)\n",
    "    \n",
    "    print(\"üíæ Results saved to: local_evaluation_results.json\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Local evaluation failed: {e}\")\n",
    "    local_result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d525400",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Cloud Evaluation\n",
    "\n",
    "Upload evaluation results to Azure AI Foundry project for tracking and collaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Evaluation - Fixed using Official Microsoft Documentation\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"‚òÅÔ∏è Setting up Cloud Evaluation following official documentation...\")\n",
    "\n",
    "# Step 1: Install and import required packages\n",
    "try:\n",
    "    from azure.ai.projects import AIProjectClient\n",
    "    from azure.ai.projects.models import (\n",
    "        EvaluatorConfiguration,\n",
    "        EvaluatorIds,\n",
    "        Evaluation,\n",
    "        InputDataset\n",
    "    )\n",
    "    print(\"‚úÖ Azure AI Projects SDK found\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Installing azure-ai-projects...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"azure-ai-projects>=1.0.0b4\"])\n",
    "    \n",
    "    from azure.ai.projects import AIProjectClient\n",
    "    from azure.ai.projects.models import (\n",
    "        EvaluatorConfiguration,\n",
    "        EvaluatorIds,\n",
    "        Evaluation,\n",
    "        InputDataset\n",
    "    )\n",
    "    print(\"‚úÖ Packages installed successfully\")\n",
    "\n",
    "# Step 2: Configuration using official environment variable names\n",
    "PROJECT_ENDPOINT = os.environ.get(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "MODEL_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\") \n",
    "MODEL_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "MODEL_DEPLOYMENT_NAME = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "\n",
    "# For cloud evaluation, convert Azure OpenAI endpoint to Azure AI services format\n",
    "# Documentation requires: https://<account>.services.ai.azure.com format\n",
    "if MODEL_ENDPOINT and \"openai.azure.com\" in MODEL_ENDPOINT:\n",
    "    # Extract account name from https://account-name.openai.azure.com/\n",
    "    account_name = MODEL_ENDPOINT.split(\"://\")[1].split(\".openai.azure.com\")[0]\n",
    "    MODEL_ENDPOINT = f\"https://{account_name}.services.ai.azure.com\"\n",
    "    print(f\"üîÑ Converted endpoint to Azure AI services format\")\n",
    "\n",
    "print(f\"üè¢ Project Endpoint: {PROJECT_ENDPOINT}\")\n",
    "print(f\"ü§ñ Model Deployment: {MODEL_DEPLOYMENT_NAME}\")\n",
    "print(f\"üîó Model Endpoint: {MODEL_ENDPOINT}\")\n",
    "\n",
    "if not PROJECT_ENDPOINT:\n",
    "    print(\"‚ö†Ô∏è Missing AZURE_AI_PROJECT_ENDPOINT in .env file\")\n",
    "    cloud_result = None\n",
    "else:\n",
    "    try:\n",
    "        # Step 3: Authentication using DefaultAzureCredential\n",
    "        print(\"üîê Setting up authentication...\")\n",
    "        credential = DefaultAzureCredential()\n",
    "        \n",
    "        # Step 4: Create AI Project Client\n",
    "        print(\"üè≠ Creating AIProjectClient...\")\n",
    "        project_client = AIProjectClient(\n",
    "            endpoint=PROJECT_ENDPOINT,\n",
    "            credential=credential,\n",
    "        )\n",
    "        print(\"‚úÖ AIProjectClient created successfully!\")\n",
    "        \n",
    "        # Step 5: Upload dataset\n",
    "        print(\"\\nüì§ Uploading evaluation data...\")\n",
    "        dataset_name = f\"health-fitness-eval-{int(time.time())}\"\n",
    "        dataset_version = \"1.0\"\n",
    "        \n",
    "        upload_response = project_client.datasets.upload_file(\n",
    "            name=dataset_name,\n",
    "            version=dataset_version,\n",
    "            file_path=str(eval_data_path),\n",
    "        )\n",
    "        data_id = upload_response.id\n",
    "        print(f\"‚úÖ Dataset uploaded with ID: {data_id}\")\n",
    "        \n",
    "        # Step 6: Configure evaluators using correct format from documentation\n",
    "        print(\"\\n‚öôÔ∏è Configuring evaluators...\")\n",
    "        evaluators = {}\n",
    "        \n",
    "        # BLEU Score - Works without AI model (mathematical evaluator)\n",
    "        evaluators[\"bleu_score\"] = EvaluatorConfiguration(\n",
    "            id=EvaluatorIds.BLEU_SCORE.value,\n",
    "            data_mapping={\n",
    "                \"prediction\": \"${data.response}\",\n",
    "                \"reference\": \"${data.ground_truth}\",\n",
    "            },\n",
    "        )\n",
    "        print(\"‚úÖ Added BLEU Score evaluator\")\n",
    "        \n",
    "        # AI-assisted evaluators (if Azure OpenAI is configured)\n",
    "        if MODEL_ENDPOINT and MODEL_API_KEY and MODEL_DEPLOYMENT_NAME:\n",
    "            # Relevance evaluator\n",
    "            evaluators[\"relevance\"] = EvaluatorConfiguration(\n",
    "                id=EvaluatorIds.RELEVANCE.value,\n",
    "                init_params={\"deployment_name\": MODEL_DEPLOYMENT_NAME},\n",
    "                data_mapping={\n",
    "                    \"query\": \"${data.query}\",\n",
    "                    \"response\": \"${data.response}\",\n",
    "                },\n",
    "            )\n",
    "            print(\"‚úÖ Added Relevance evaluator\")\n",
    "            \n",
    "            # Coherence evaluator  \n",
    "            evaluators[\"coherence\"] = EvaluatorConfiguration(\n",
    "                id=EvaluatorIds.COHERENCE.value,\n",
    "                init_params={\"deployment_name\": MODEL_DEPLOYMENT_NAME},\n",
    "                data_mapping={\n",
    "                    \"query\": \"${data.query}\",\n",
    "                    \"response\": \"${data.response}\",\n",
    "                },\n",
    "            )\n",
    "            print(\"‚úÖ Added Coherence evaluator\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Azure OpenAI not configured - using BLEU Score only\")\n",
    "        \n",
    "        # Step 7: Create and submit evaluation\n",
    "        print(\"\\nüöÄ Submitting cloud evaluation...\")\n",
    "        eval_display_name = f\"Health-Fitness-Eval-{int(time.time())}\"\n",
    "        \n",
    "        evaluation = Evaluation(\n",
    "            display_name=eval_display_name,\n",
    "            description=\"Health & Fitness Q&A Evaluation - Fixed Implementation\",\n",
    "            data=InputDataset(id=data_id),\n",
    "            evaluators=evaluators,\n",
    "        )\n",
    "        \n",
    "        # Add headers for AI evaluators as per documentation\n",
    "        headers = {}\n",
    "        if MODEL_ENDPOINT and MODEL_API_KEY:\n",
    "            headers = {\n",
    "                \"model-endpoint\": MODEL_ENDPOINT,\n",
    "                \"api-key\": MODEL_API_KEY,\n",
    "            }\n",
    "            print(\"‚úÖ Added model configuration headers\")\n",
    "        \n",
    "        evaluation_response = project_client.evaluations.create(\n",
    "            evaluation,\n",
    "            headers=headers if headers else None,\n",
    "        )\n",
    "        \n",
    "        print(\"üéâ CLOUD EVALUATION SUBMITTED!\")\n",
    "        print(f\"   üìã Name: {evaluation_response.name}\")\n",
    "        print(f\"   ‚è≥ Status: {evaluation_response.status}\")\n",
    "        \n",
    "        # Brief status check\n",
    "        print(\"\\n‚è≥ Checking initial status...\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        try:\n",
    "            status = project_client.evaluations.get(evaluation_response.name)\n",
    "            print(f\"üìä Current Status: {status.status}\")\n",
    "        except Exception:\n",
    "            print(\"‚ö†Ô∏è Could not retrieve status update\")\n",
    "        \n",
    "        print(f\"üîó View results: https://ai.azure.com/\")\n",
    "        \n",
    "        # Save results\n",
    "        cloud_result = {\n",
    "            \"evaluation_name\": evaluation_response.name,\n",
    "            \"status\": evaluation_response.status,\n",
    "            \"dataset_id\": data_id,\n",
    "            \"evaluators\": list(evaluators.keys()),\n",
    "            \"project_endpoint\": PROJECT_ENDPOINT,\n",
    "            \"timestamp\": int(time.time())\n",
    "        }\n",
    "        \n",
    "        with open(\"cloud_evaluation_results.json\", \"w\") as f:\n",
    "            json.dump(cloud_result, f, indent=2)\n",
    "        print(\"üíæ Results saved to: cloud_evaluation_results.json\")\n",
    "        \n",
    "        print(\"\\n‚úÖ SUCCESS: Cloud evaluation is running!\")\n",
    "        print(\"üí° Check Azure AI Studio portal for detailed results\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Cloud evaluation failed: {e}\")\n",
    "        print(f\"üìã Error type: {type(e).__name__}\")\n",
    "        \n",
    "        # Enhanced error handling\n",
    "        error_str = str(e).lower()\n",
    "        if \"401\" in error_str or \"unauthorized\" in error_str:\n",
    "            print(\"\\nüîê AUTHENTICATION ISSUE:\")\n",
    "            print(\"   - Run 'az login' in terminal\")\n",
    "            print(\"   - Ensure you're logged in to correct tenant\")\n",
    "        elif \"403\" in error_str or \"forbidden\" in error_str:\n",
    "            print(\"\\nüö´ PERMISSION ISSUE:\")\n",
    "            if \"storage\" in error_str:\n",
    "                print(\"   - Storage account needs proper permissions\")\n",
    "                print(\"   - Add 'Storage Blob Data Contributor' role to project MSI\")\n",
    "            else:\n",
    "                print(\"   - Check project permissions\")\n",
    "                print(\"   - Verify you have access to the Azure AI project\")\n",
    "        elif \"404\" in error_str or \"not found\" in error_str:\n",
    "            print(\"\\nüîç RESOURCE NOT FOUND:\")\n",
    "            print(\"   - Verify project endpoint is correct\")\n",
    "            print(\"   - Check if project exists in Azure AI Foundry\")\n",
    "        else:\n",
    "            print(f\"\\nüí° TROUBLESHOOTING:\")\n",
    "            print(f\"   - Full error: {str(e)[:200]}...\")\n",
    "            print(\"   - Check network connectivity\")\n",
    "            print(\"   - Verify all environment variables\")\n",
    "        \n",
    "        cloud_result = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "end-to-end-ai-foundry-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
